{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c8ecc7",
   "metadata": {},
   "source": [
    "# Classifies the Project CETI sperm whale audio by coda type\n",
    "\n",
    "<html><a href=\"https://colab.research.google.com/github/autumnjohnson/ceti_audio/blob/main/load_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></html>\n",
    "\n",
    "\n",
    "Hugging Face: <https://huggingface.co/datasets/autumnjohnson/ceti_audio>\n",
    "\n",
    "GitHub: <https://github.com/autumnjohnson/ceti_audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e39461",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2701f0c8-15ce-4135-a02a-4946d0730d33",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
   ],
   "source": [
    "!pip install matplotlib torch pydub datasets\n",
    "import io\n",
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import numpy.core.multiarray as multi\n",
    "\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "from collections import Counter\n",
    "import gzip\n",
    "\n",
    "\n",
    "from IPython.display import Audio as iAudio\n",
    "from npc_gzip.compressors.base import BaseCompressor\n",
    "from npc_gzip.compressors.bz2_compressor import Bz2Compressor\n",
    "from npc_gzip.compressors.gzip_compressor import GZipCompressor\n",
    "from npc_gzip.knn_classifier import KnnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "384ef3fd-7743-4256-aa2e-6efa750fb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = GZipCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2d292a7-eab3-4ca4-994d-db6de76d2268",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(comp: BaseCompressor, train_audio: np.ndarray, train_labels: np.ndarray, distance_metric: str = \"ncd\") -> KnnClassifier:\n",
    "    \"\"\"\n",
    "    Fits a Knn-GZip compressor on the train\n",
    "    data and returns it.\n",
    "\n",
    "    Arguments:\n",
    "        train_text (np.ndarray): Training dataset as a numpy array.\n",
    "        train_labels (np.ndarray): Training labels as a numpy array.\n",
    "\n",
    "    Returns:\n",
    "        KnnClassifier: Trained Knn-Compressor model ready to make predictions.\n",
    "    \"\"\"\n",
    "    compressor: BaseCompressor = comp\n",
    "    model: KnnClassifier = KnnClassifier(\n",
    "        compressor=compressor,\n",
    "        training_inputs=train_audio,\n",
    "        training_labels=train_labels,\n",
    "        distance_metric=distance_metric,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540aa41",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69cce",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "605b895e-ffe1-4b63-aceb-6752ea5adabe",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"autumnjohnson/ceti_audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ca2b0d",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2590c",
   "metadata": {},
   "source": [
    "### Resample audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "845cb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(decode=False, sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49329dbe",
   "metadata": {},
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47a3f9b1-abf7-4c42-bf6a-ac1aaa421eb1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset) -> tuple:\n",
    "    \"\"\"\n",
    "    Pulls the Project CETI sperm whale vocalizations\n",
    "    training data and the second being the test\n",
    "    data. Each tuple contains the audio and label\n",
    "    respectively as numpy arrays.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_iter = dataset['train']\n",
    "    test_iter = dataset['test']\n",
    "\n",
    "    train_audio =  [audio_array['bytes'] for audio_array in train_iter['audio']]\n",
    "    train_labels = train_iter['coda_type']\n",
    "\n",
    "    test_audio  = [audio_array['bytes'] for audio_array in test_iter['audio']]\n",
    "    test_labels = test_iter['coda_type']\n",
    "\n",
    "    train_audio = np.array(train_audio)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    test_audio = np.array(test_audio)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    train = (train_audio, train_labels)\n",
    "    test = (test_audio, test_labels)\n",
    "\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec38ebc0-04f2-4738-8382-a8b11bb9837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_audio, train_labels), (test_audio, test_labels)) = get_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c44dab-06b9-4b4e-8cfd-c22986ea4de8",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4b8b5d2-0557-4e5a-9944-ad515810dd6d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
   ],
   "source": [
    "model = fit_model(compressor, train_audio, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e29f09",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "854af5c7-92fe-4491-a223-48443be667ee",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sets the percent of training set to use to compare `sample_test_text` against\n",
    "# A value less than 1 saves time at the expense of worse predictions\n",
    "sampling_percentage = 1\n",
    "top_k = 1\n",
    "random_indicies = np.random.choice(test_audio.shape[0], len(test_audio), replace=False)\n",
    "sample_test_text = test_audio[random_indicies]\n",
    "sample_test_labels = test_labels[random_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01833d34",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "(distances, labels, similar_samples) = model.predict(sample_test_text, top_k,\n",
    "sampling_percentage=sampling_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e7e3b7c-edf3-4f86-a35b-62110b7d673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_labels = labels.flatten().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a138d",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bdad0",
   "metadata": {},
   "source": [
    "### Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2037a248-2e87-4c50-bdd8-cd58c66e491e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
 
   ],
   "source": [
    "print(classification_report(sample_test_labels, flattened_labels, zero_division=np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfe048",
   "metadata": {},
   "source": [
    "### Print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "671995ba-da3d-43bc-80cb-a00242440fc3",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(sample_test_labels, flattened_labels)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dac34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
